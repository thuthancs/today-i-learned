# Why is Python so slow?
<img width="897" height="102" alt="Screenshot 2026-02-17 at 10 52 53â€¯PM" src="https://github.com/user-attachments/assets/9bad1574-94e7-4f8f-a897-93e33ec90f2d" />

I tried running a prime-number checker function on both C++ and Python (the number I checked was 99999999989). The result is that C++ is 10x faster than Python.

## What happen when you run a program?
This is taken from the book "High Performance Python". The way the book explains "what happen when..." you run a program is as follows:
- Create a function `f(number)` to check whether it is a prime number or not.
- When we pass in a specific number, say `1001`, and start the code, the value of the variable `number` is stored in RAM (Random-Accessed Memory).
- The value will then be stored inside the CPU's L1/L2 cache (I don't really understand this yet) and the CPU will perform the calculations.
- Then, the CPU will send back the value to RAM to get stored.
- Because the CPU has this ability to vectorize a calculation (i.e., doing multiple independent computations at the same time), ideally, we would want to send multipl values of `i` of a for-loop to get multiple calculations done.

## Why Python is not as fast as C++?
### Global Interpreter Lock (GIL)
<img width="5504" height="2186" alt="gil_illustration" src="https://github.com/user-attachments/assets/1cffc6a3-47a7-4633-bb57-9a3d28bb4019" />

- Python uses **GIL**, a lock (i.e., mutex - **mutal exclusion lock**) in the default CPython interpreter that ensures _**only one thread** of Python bytecode executes at a time_. This creates a tradeoff between memory management simplication and parallelism capacity limitation for CPU-bound tasks in standard Python multithreading.
  - This makes tasks such as mathematical calculations or image processing very slow using Python because the GIL prevents threads from running in parallel on multiple CPU cores.
  - However, it ensures thread safety because GIL prevents multiple threads from executing Python bytecode simultaneously within the same proces, which prevents memory corruption of the interpreter's internal state + the operations are atomic.
- There is an example from [1] that I really like: Image a team of 3 asking people to fill out a survey. We need to collect 100 responses. Each team member takes 1 minute to collect 1 survey. If one team member is done asking 1 person, he/she can just go on and ask the next person. This means there is no constraint in terms of sequential order. In this case, adding more people, let's say 5-10 more, is beneficial because of this kind of distribution. However, imagine team member B can only ask another person to fill the survey after team member A is done, then adding more people (having more cores) does not guarantee the team will achieve the goal faster.
  
### Compiled vs. Interpreted
- Python is:
  - **Interpreted**: This requires more step at runtime: Source code -> compiled into bytecode -> interpreted by a virtual machine (CPython) at runtime.
  - **Dynamically typed**: variable types can change at runtime, requiring type checks and dynamic lookups for every operation.
  - **High-level**: abstract many hardware details
 
### Garbage Collector
- Memory is automatically allocated and freed when needed -> creates memory fragmentation that can hurt the transfers to the CPU caches.
  
## Workaround & Examples
- `multiprocessing`
- C Extensions


## References
[1] High Performance Python: Practical Performant Programming for Humans (Ian Ozsvald and Micha Gorelick)
